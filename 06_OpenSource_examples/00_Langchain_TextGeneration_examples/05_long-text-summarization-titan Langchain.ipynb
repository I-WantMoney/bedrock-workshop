{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fded102b",
   "metadata": {},
   "source": [
    "# Abstractive Text Summarization with Amazon Titan\n",
    "\n",
    "> *This notebook should work well with the **`Data Science 3.0`** kernel in SageMaker Studio*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab8b2cf",
   "metadata": {},
   "source": [
    "## Overview\n",
    "When we work with large documents, we can face some challenges as the input text might not fit into the model's context window, or the model hallucinates with large documents, or out of memory errors occur, etc.\n",
    "\n",
    "To solve those problems, we are going to show an architecture that is based on the concept of chunking and chaining prompts. This architecture is leveraging [LangChain](https://python.langchain.com/docs/get_started/introduction.html) which is a popular framework for developing applications powered by language models.\n",
    "\n",
    "### Architecture\n",
    "\n",
    "![](../../imgs/42-text-summarization-2.png)\n",
    "\n",
    "In this architecture:\n",
    "\n",
    "1. A large document (or a giant file appending small ones) is loaded\n",
    "2. A LangChain utility is used to split it into multiple smaller chunks (chunking)\n",
    "3. The first chunk is sent to the model; the model returns the corresponding summary\n",
    "4. LangChain gets next chunk and appends it to the returned summary and sends the combined text as a new request to the model; the process repeats until all chunks are processed\n",
    "5. In the end, you have a final summary based on entire content\n",
    "\n",
    "### Use case\n",
    "This approach can be used to summarize call transcripts, meetings transcripts, books, articles, blog posts, and other relevant content."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b0be05-906a-41b7-984f-ed6cd7fd8006",
   "metadata": {},
   "source": [
    "### Pre-requisites\n",
    "\n",
    "Install LangChain pre-requisites"
   ]
  },
  {
   "cell_type": "code",
   "id": "983cc30f-d1ae-4b27-bce8-ce40f5393720",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-04-30T18:14:38.625034Z",
     "start_time": "2024-04-30T18:14:29.041101Z"
    }
   },
   "source": [
    "%pip install -U --no-cache-dir boto3\n",
    "%pip install -U --no-cache-dir  \\\n",
    "    \"langchain>=0.1.11\" \\\n",
    "    lanchain-aws==0.1.0 \\\n",
    "    sqlalchemy -U \\\n",
    "    \"faiss-cpu>=1.7,<2\" \\\n",
    "    \"pypdf>=3.8,<4\" \\\n",
    "    pinecone-client==2.2.4 \\\n",
    "    apache-beam==2.52. \\\n",
    "    tiktoken==0.5.2 \\\n",
    "    \"ipywidgets>=7,<8\" \\\n",
    "    matplotlib==3.8.2 \\\n",
    "    anthropic==0.9.0\n",
    "%pip install -U --no-cache-dir transformers"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: boto3 in /Users/jicowan/PycharmProjects/amazon-bedrock-workshop/.venv/lib/python3.11/site-packages (1.34.94)\r\n",
      "Requirement already satisfied: botocore<1.35.0,>=1.34.94 in /Users/jicowan/PycharmProjects/amazon-bedrock-workshop/.venv/lib/python3.11/site-packages (from boto3) (1.34.94)\r\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /Users/jicowan/PycharmProjects/amazon-bedrock-workshop/.venv/lib/python3.11/site-packages (from boto3) (1.0.1)\r\n",
      "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /Users/jicowan/PycharmProjects/amazon-bedrock-workshop/.venv/lib/python3.11/site-packages (from boto3) (0.10.1)\r\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /Users/jicowan/PycharmProjects/amazon-bedrock-workshop/.venv/lib/python3.11/site-packages (from botocore<1.35.0,>=1.34.94->boto3) (2.9.0.post0)\r\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /Users/jicowan/PycharmProjects/amazon-bedrock-workshop/.venv/lib/python3.11/site-packages (from botocore<1.35.0,>=1.34.94->boto3) (2.2.1)\r\n",
      "Requirement already satisfied: six>=1.5 in /Users/jicowan/PycharmProjects/amazon-bedrock-workshop/.venv/lib/python3.11/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.35.0,>=1.34.94->boto3) (1.16.0)\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain>=0.1.11 in /Users/jicowan/PycharmProjects/amazon-bedrock-workshop/.venv/lib/python3.11/site-packages (0.1.16)\r\n",
      "\u001B[31mERROR: Could not find a version that satisfies the requirement lanchain-aws==0.1.0 (from versions: none)\u001B[0m\u001B[31m\r\n",
      "\u001B[0m\u001B[31mERROR: No matching distribution found for lanchain-aws==0.1.0\u001B[0m\u001B[31m\r\n",
      "\u001B[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Users/jicowan/PycharmProjects/amazon-bedrock-workshop/.venv/lib/python3.11/site-packages (4.40.1)\r\n",
      "Requirement already satisfied: filelock in /Users/jicowan/PycharmProjects/amazon-bedrock-workshop/.venv/lib/python3.11/site-packages (from transformers) (3.13.4)\r\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /Users/jicowan/PycharmProjects/amazon-bedrock-workshop/.venv/lib/python3.11/site-packages (from transformers) (0.22.2)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/jicowan/PycharmProjects/amazon-bedrock-workshop/.venv/lib/python3.11/site-packages (from transformers) (1.24.4)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/jicowan/PycharmProjects/amazon-bedrock-workshop/.venv/lib/python3.11/site-packages (from transformers) (23.2)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/jicowan/PycharmProjects/amazon-bedrock-workshop/.venv/lib/python3.11/site-packages (from transformers) (6.0.1)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/jicowan/PycharmProjects/amazon-bedrock-workshop/.venv/lib/python3.11/site-packages (from transformers) (2024.4.16)\r\n",
      "Requirement already satisfied: requests in /Users/jicowan/PycharmProjects/amazon-bedrock-workshop/.venv/lib/python3.11/site-packages (from transformers) (2.31.0)\r\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /Users/jicowan/PycharmProjects/amazon-bedrock-workshop/.venv/lib/python3.11/site-packages (from transformers) (0.19.1)\r\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Users/jicowan/PycharmProjects/amazon-bedrock-workshop/.venv/lib/python3.11/site-packages (from transformers) (0.4.3)\r\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/jicowan/PycharmProjects/amazon-bedrock-workshop/.venv/lib/python3.11/site-packages (from transformers) (4.66.2)\r\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/jicowan/PycharmProjects/amazon-bedrock-workshop/.venv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2024.3.1)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/jicowan/PycharmProjects/amazon-bedrock-workshop/.venv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.11.0)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/jicowan/PycharmProjects/amazon-bedrock-workshop/.venv/lib/python3.11/site-packages (from requests->transformers) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/jicowan/PycharmProjects/amazon-bedrock-workshop/.venv/lib/python3.11/site-packages (from requests->transformers) (3.7)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/jicowan/PycharmProjects/amazon-bedrock-workshop/.venv/lib/python3.11/site-packages (from requests->transformers) (2.2.1)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/jicowan/PycharmProjects/amazon-bedrock-workshop/.venv/lib/python3.11/site-packages (from requests->transformers) (2024.2.2)\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "id": "fcc7dfe4",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "⚠️ ⚠️ ⚠️ Before running this notebook, ensure you've run the [Bedrock boto3 setup notebook](../00_Intro/bedrock_boto3_setup.ipynb#Prerequisites) notebook. ⚠️ ⚠️ ⚠️\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ae9a41",
   "metadata": {},
   "source": [
    "## Summarize long text \n",
    "\n",
    "### Configuring LangChain with Boto3\n",
    "\n",
    "LangChain automatically passes boto3 session information to LangChain from your environment.\n",
    "\n",
    "You need to specify an LLM for the LangChain BedrockLLM class. You can also pass in arguments for inference such as temperature and top_p. Here you specify Amazon Titan Text Large in `model_id` and pass Titan's inference parameter in `textGenerationConfig`."
   ]
  },
  {
   "cell_type": "code",
   "id": "93df2442",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-04-30T18:14:38.987049Z",
     "start_time": "2024-04-30T18:14:38.629305Z"
    }
   },
   "source": [
    "from langchain_aws import BedrockLLM\n",
    "\n",
    "modelId = \"amazon.titan-tg1-large\"\n",
    "llm = BedrockLLM(\n",
    "    model_id=modelId,\n",
    "    model_kwargs={\n",
    "        \"maxTokenCount\": 4096,\n",
    "        \"stopSequences\": [],\n",
    "        \"temperature\": 0,\n",
    "        \"topP\": 1,\n",
    "    },\n",
    "    # client=boto3_bedrock,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "id": "31223056",
   "metadata": {},
   "source": [
    "### Loading a text file with many tokens\n",
    "\n",
    "In `letters` directory, you can find a text file of [Amazon's CEO letter to shareholders in 2022](https://www.aboutamazon.com/news/company-news/amazon-ceo-andy-jassy-2022-letter-to-shareholders). The following cell loads the text file and counts the number of tokens in the file. \n",
    "\n",
    "You will see warning indicating the number of tokens in the text file exceeeds the maximum number of tokens for this model."
   ]
  },
  {
   "cell_type": "code",
   "id": "c70352ae",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-04-30T18:14:39.016382Z",
     "start_time": "2024-04-30T18:14:38.989627Z"
    }
   },
   "source": [
    "# You may have to run this block twice\n",
    "shareholder_letter = \"./letters/2022-letter.txt\"\n",
    "\n",
    "with open(shareholder_letter, \"r\") as file:\n",
    "    letter = file.read()\n",
    "    \n",
    "llm.get_num_tokens(letter)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6526"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "id": "dc8ec39d",
   "metadata": {},
   "source": [
    "### Splitting the long text into chunks\n",
    "\n",
    "The text is too long to fit in the context windows for the Titan model we've chosen, so we will split it into smaller chunks.\n",
    "`RecursiveCharacterTextSplitter` in LangChain supports splitting long text into chunks recursively until size of each chunk becomes smaller than `chunk_size`. A text is separated with `separators=[\"\\n\\n\", \"\\n\"]` into chunks, which avoids splitting each paragraph into multiple chunks.\n",
    "\n",
    "Using 6,000 characters per chunk, we can get summaries for each portion separately. The number of tokens, or word pieces, in a chunk depends on the text."
   ]
  },
  {
   "cell_type": "code",
   "id": "2e7c372b",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-04-30T18:14:39.024789Z",
     "start_time": "2024-04-30T18:14:39.019569Z"
    }
   },
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=[\"\\n\\n\", \"\\n\"], chunk_size=4000, chunk_overlap=100\n",
    ")\n",
    "\n",
    "docs = text_splitter.create_documents([letter])"
   ],
   "outputs": [],
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "id": "f66569f0",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-04-30T18:14:39.034984Z",
     "start_time": "2024-04-30T18:14:39.026724Z"
    }
   },
   "source": [
    "num_docs = len(docs)\n",
    "\n",
    "num_tokens_first_doc = llm.get_num_tokens(docs[0].page_content)\n",
    "\n",
    "print(\n",
    "    f\"Now we have {num_docs} documents and the first one has {num_tokens_first_doc} tokens\"\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now we have 10 documents and the first one has 439 tokens\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "cell_type": "markdown",
   "id": "a5f8ae45",
   "metadata": {},
   "source": [
    "### Summarizing chunks and combining them"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61d49f5",
   "metadata": {},
   "source": [
    "Assuming that the number of tokens is consistent in the other docs we should be good to go. Let's use LangChain's [load_summarize_chain](https://python.langchain.com/en/latest/use_cases/summarization.html) to summarize the text. `load_summarize_chain` provides three ways of summarization: `stuff`, `map_reduce`, and `refine`. \n",
    "- `stuff` puts all the chunks into one prompt. Thus, this would hit the maximum limit of tokens.\n",
    "- `map_reduce` summarizes each chunk, combines the summary, and summarizes the combined summary. If the combined summary is too large, it would raise error.\n",
    "- `refine` summarizes the first chunk, and then summarizes the second chunk with the first summary. The same process repeats until all chunks are summarized.\n",
    "\n",
    "`map_reduce` and `refine` invoke LLM multiple times and takes time to obtain the final summary. \n",
    "Let's try `map_reduce` here. "
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T18:44:07.471764Z",
     "start_time": "2024-04-30T18:44:07.467084Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain import PromptTemplate\n",
    "map_prompt_template = \"\"\"Write a summary of this chuck of text that includes the main points and any important details: {text}\"\"\"\n",
    "map_prompt = PromptTemplate(template=map_prompt_template, input_variables=[\"text\"])\n",
    "combined_prompt_template = \"\"\"Write a concise summary of the following text: {text}\"\"\"\n",
    "combined_prompt = PromptTemplate(template=combined_prompt_template, input_variables=[\"text\"])"
   ],
   "id": "7061c047cc20add7",
   "outputs": [],
   "execution_count": 33
  },
  {
   "cell_type": "code",
   "id": "b3b08c54",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-04-30T18:51:48.544199Z",
     "start_time": "2024-04-30T18:51:48.541127Z"
    }
   },
   "source": [
    "# Set verbose=True if you want to see the prompts being used\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "summary_chain = load_summarize_chain(llm=llm, chain_type=\"map_reduce\", verbose=False, map_prompt=map_prompt, combine_prompt=combined_prompt)"
   ],
   "outputs": [],
   "execution_count": 38
  },
  {
   "cell_type": "markdown",
   "id": "4f0eda5e-36a5-4618-ac5a-e272673d6f26",
   "metadata": {},
   "source": [
    "> ⏰ **Note:** Depending on your number of documents, Bedrock request rate quota, and configured retry settings - the chain below may take some time to run."
   ]
  },
  {
   "cell_type": "code",
   "id": "ba73121e",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-04-30T18:54:19.230550Z",
     "start_time": "2024-04-30T18:52:00.201849Z"
    }
   },
   "source": [
    "%%time\n",
    "output = \"\"\n",
    "try:\n",
    "    output = summary_chain.invoke({\"input_documents\": docs})\n",
    "except ValueError as error:\n",
    "    if  \"AccessDeniedException\" in str(error):\n",
    "        print(f\"\\x1b[41m{error}\\\n",
    "        \\nTo troubleshoot this issue please refer to the following resources.\\\n",
    "         \\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/troubleshoot_access-denied.html\\\n",
    "         \\nhttps://docs.aws.amazon.com/bedrock/latest/userguide/security-iam.html\\x1b[0m\\n\")      \n",
    "        class StopExecution(ValueError):\n",
    "            def _render_traceback_(self):\n",
    "                pass\n",
    "        raise StopExecution        \n",
    "    else:\n",
    "        raise error"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 58.4 ms, sys: 11.2 ms, total: 69.6 ms\n",
      "Wall time: 2min 19s\n"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T18:59:03.914363Z",
     "start_time": "2024-04-30T18:59:03.907088Z"
    }
   },
   "cell_type": "code",
   "source": "output.get('output_text')",
   "id": "428378f084ad8bbd",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nAmazon has had a successful year despite facing challenges in 2022. The company has grown demand, innovated in its largest businesses, and made adjustments to its investment decisions. Amazon operates in large, dynamic, global market segments with many capable and well-funded competitors, and the company has experienced constant change over the past 25 years. In 1997, Amazon was a books-only retailer, and today it sells nearly every physical and digital retail item. Similarly, building a business around technology infrastructure services in the cloud was not obvious in 2003, and Amazon has since launched AWS and Kindle.\\n\\nAWS has made significant structural changes to deliver lower costs and faster speed. This included reevaluating the US fulfillment network, which had one national network that distributed inventory from fulfillment centers spread across the country. Last year, AWS started rearchitecting its inventory placement strategy and leveraging its larger fulfillment center footprint to move from a national fulfillment network to a regionalized network model. The regional roll out has recently completed and early results show shorter travel distances, lower cost to serve, less impact on the environment, and customers getting their orders faster. AWS remains confident about its plans to lower costs, reduce delivery times, and build a meaningfully larger retail business with healthy operating margins.\\n\\nAWS has an $85B annualized revenue run rate, is still early in its adoption curve, but at a juncture where it’s critical to stay focused on what matters most to customers over the long-haul. Despite growing 29% year-over-year in 2022 on a $62B revenue base, AWS faces short-term headwinds right now as companies are being more cautious in spending given the challenging, current macroeconomic conditions. While some companies might obsess over how they could extract as much money from customers as possible in these tight times, it’s neither what customers want nor best for customers in the long term, so AWS is taking a different tack. One of the many advantages of AWS and cloud computing is that when your business grows, you can seamlessly scale up; and conversely, if your business contracts, you can choose to give us back that capacity and cease paying for it. This elasticity is unique to the cloud, and doesn’t exist when you’ve already made expensive capital investments in your own on-premises datacenters, servers, and networking gear. In AWS, like all our businesses, we’re not trying to optimize for any one quarter or year. We’re trying to build customer relationships (and a business) that outlast all of us; and as a result, our AWS sales and support teams are spending much of their time helping customers optimize their AWS spend so they can better weather this uncertain economy. Many of these AWS customers tell us that they’re not cost-cutting as much as cost-optimizing so they can take their resources and apply them to emerging and inventive new customer experiences they’re planning. Customers have appreciated this customer-focused, long-term approach, and AWS thinks it’ll bode well for both customers and AWS.\\n\\nWhile AWS faces short-term headwinds, it remains strong with a robust customer pipeline, active migrations, and enterprises opting out of managing their infrastructure. AWS continues to deliver new capabilities rapidly and invest in long-term inventions, such as Graviton2-based compute instances, Graviton3 chips, and Trainium-based instances for machine learning training and inference. In 2022, AWS launched its first training chip, Trainium, and its first inference chips, Inferentia, which have saved companies like Amazon over a hundred million dollars in capital expense. The Inferentia2 chip offers up to four times higher throughput and ten times lower latency than the first Inferentia processor, and AWS is not close to being done innovating. Amazon's Advertising business is also uniquely effective for brands, with sponsored products and brands offerings tailored to what customers are searching for. This leads to advertising that's more useful for customers and performs better for brands, with Advertising revenue growing rapidly (23% YoY in Q4 2022, 25% YoY overall for 2022 on a $31B revenue base).\\n\\nAmazon is committed to being the best place for advertisers to build their brands. They are investing in machine learning to improve their advertising selection algorithms and have built comprehensive, flexible, and durable planning and measurement solutions. Amazon Marketing Cloud (AMC) is a secure digital environment where advertisers can run custom audience and campaign analytics across first and third-party inputs to generate advertising and business insights. The Advertising and AWS teams have collaborated to enable companies to store their data in AWS, operate securely in AMC, perform analytics in AWS, and activate advertising on Amazon or third-party publishers through the Amazon Demand-Side Platform. Customers appreciate the concerted capability and see future opportunity to integrate advertising into their video, live sports, audio, and grocery products. Amazon is committed to helping brands uniquely engage with the products and grow this part of their business. When looking at new investment opportunities, they ask themselves if they could be big and have a reasonable return on invested capital, if the opportunity is being well-served today, if they have a differentiated approach, and if they have competence in that area. Examples of expansions include expanding from just selling Books to adding categories like Music, Video, Electronics, and Toys, and expanding international Stores. In 2022, the international consumer segment drove $118B of revenue, with growth in 2019-2021 of 30% CAGR in the UK, 26% in Germany, and 21% in Japan. Amazon has invested in new international geographies, including India, Brazil, Mexico, Australia, various European countries, the Middle East, and parts of Africa, to serve a broader geographical footprint and help more customers across the world. They continue to work with partners to deliver solutions for customers.\\n\\nAmazon has been working to expand its customer offerings across large, unique product retail market segments, such as grocery. Amazon has built a significant grocery business over nearly 20 years, offering more than three million items compared to a typical supermarket's 30K. To serve more of its customers' grocery needs, Amazon needs a broader physical store footprint. Whole Foods Market pioneered the natural and organic specialty grocery store concept 40 years ago and is a large and growing business. Amazon Fresh is the brand they've been experimenting with for a few years and are working hard to identify and build the right mass grocery format for Amazon scale. Amazon Business is another example of an investment where their ecommerce and logistics capabilities position them well to pursue this large market segment. It launched in 2015 and today drives roughly $35B in annualized gross sales.\\n\\nAmazon has developed Buy with Prime to assist third-party brands and sellers in offering their products on their own websites to Amazon Prime members. It provides merchants with several additional benefits, including Amazon handling product storage, picking, packing, delivery, payment, and returns. Buy with Prime has increased shopper conversion on third-party shopping sites by 25% on average, and merchants are excited about converting more sales and fulfilling shipments more easily. Amazon is also expanding internationally, pursuing large retail market segments, and using its unique assets to help merchants sell more effectively on their own websites. In 2003, AWS would have been a classic example, while Amazon Healthcare and Kuiper are potential analogues in 2023. Amazon Pharmacy is a full-service, online pharmacy that offers transparent pricing, easy refills, and savings for Prime members. However, customers have expressed a strong desire for Amazon to provide a better alternative to the inefficient and unsatisfying broader healthcare experience. In July 2022, Amazon announced its acquisition of One Medical, a patient-focused experience with a digital app and offices in cities across the US. One Medical has relationships with specialty physicians and works closely with local hospital systems to make seeing specialists easy. Amazon and One Medical will continue to innovate together to change what primary care will look like for customers.\\n\\nAmazon is creating Kuiper, a low-Earth orbit satellite system to deliver high-quality broadband internet service to places around the world that don't currently have it. The system will deliver not only accessibility but affordability, with low-cost antennas that will lower the barriers to access. The company is preparing to launch two prototype satellites to test the entire end-to-end communications network this year, and plan to be in beta with commercial customers in 2024. The customer reaction to what they've shared thus far about Kuiper has been positive, and it represents a large potential opportunity for Amazon.\\n\\nAmazon is investing heavily in Large Language Models (LLMs) and Generative AI, which are based on very Large Language Models (trained on up to hundreds of billions of parameters, and growing). This technology has the potential to transform and improve virtually every customer experience, and AWS is offering the most price-performant machine learning chips in Trainium and Inferentia so small and large companies can afford to train and run their LLMs in production. LLMs and Generative AI are going to be a big deal for customers, our shareholders, and Amazon, and AWS is delivering applications like AWS’s CodeWhisperer, which revolutionizes developer productivity by generating code suggestions in real time. I am optimistic that we will emerge from this challenging macroeconomic time in a stronger position than when we entered it, with a consumer business that’s $434B in 2022 and a vast majority of total market segment share in global retail and Global IT spending still on-premises. I strongly believe that our best days are in front of us, and I look forward to working with my teammates at Amazon to make it so.\""
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 46
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "4100d0b81c85c15f"
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
