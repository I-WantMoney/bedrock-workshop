{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain と pinecone を使った Amazon Bedrock による拡張質問と回答の取得\n",
    "\n",
    "> *このノートブックは SageMaker Studio の **`Data Science 3.0`** カーネルをご利用ください*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### コンテキスト\n",
    "以前、モデルがタイヤの交換方法を教えてくれるのを見ましたが、関連データを手動で提供し、コンテキストを自分たちで提供する必要がありました。 Bedrockで利用できるモデルを活用し、トレーニング中に学んだ知識に基づいて質問したり、マニュアルコンテキストを提供したりするアプローチを検討しました。このアプローチは短い文書やシングルトンアプリケーションでは有効ですが、モデルに送られるプロンプトにすべて収まらない大規模な企業文書が存在する場合に、企業レベルの質問への回答には対応できません。 \n",
    "\n",
    "### パターン\n",
    "私たちは上記プロセスを Retreival Augmented Generation (RAG)と呼ばれるアーキテクチャを実装することで改善することができます。RAG は言語モデルの外部からデータ（ノンパラメトリック）を取得し、関連する取得データをコンテキストに追加することでプロンプトを補強します。\n",
    "\n",
    "このノートブックでは、ユーザーの質問に対する回答を提供するためにドキュメントを検索し、活用する質問応答のパターンへのアプローチ方法を説明します。\n",
    "\n",
    "### 課題\n",
    "- トークン制限を超える大きな文書をどのように管理するか\n",
    "- 質問に関連する文書を見つける方法\n",
    "\n",
    "### 提案\n",
    "上記の課題に対して、このノートでは以下の戦略を提案します。\n",
    "#### ドキュメントを準備する\n",
    "![埋め込み(Embeddings)](./images/Embeddings_lang.png)\n",
    "\n",
    "質問に答えるには、文書を処理してドキュメントストアインデックスに保存する必要があります。\n",
    "- 文書をロード\n",
    "- 処理して小さなチャンクに分割\n",
    "- Amazon Bedrock Titan Embeddings モデルを使用して、各チャンクの数値ベクトル表現を作成\n",
    "- チャンクとそれに対応する埋め込みを使用してインデックスを作成\n",
    "#### 質問する\n",
    "![質問](./images/Chatbot_lang.png)\n",
    "\n",
    "ドキュメントインデックスが作成されたら、質問する準備が整い、質問に基づいて関連ドキュメントが取得されます。 以下のステップが実行されます。\n",
    "- 入力された質問の埋め込みを作成\n",
    "- 質問の埋め込みと索引の埋め込みを比較\n",
    "- (上位 N 件) の関連文書チャンクを取得\n",
    "- それらのチャンクをプロンプトのコンテキストの一部として追加\n",
    "- Amazon Bedrock 下のモデルにプロンプトを送信\n",
    "- 取得したドキュメントに基づいて状況に応じた回答を得る"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ユースケース\n",
    "#### データセット\n",
    "このアーキテクチャパターンを説明するために、IRS の文書を使用しています。これらの文書には次のようなトピックが説明されています:\n",
    "- オリジナル・イシュー・ディスカウント (OID) 商品\n",
    "- 1万ドルを超える現金支払いのIRSへの報告\n",
    "- 雇用者向け税務ガイド\n",
    "\n",
    "#### ペルソナ\n",
    "IRS の仕組みや、何らかの行動が意味を持つかどうかを理解していない素人のペルソナを想定してみましょう。\n",
    "\n",
    "モデルは簡単な言葉で文書から答えようとします。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 実装\n",
    "RAGアプローチを採用するために、このノートブックは LangChain フレームワークを使用しています。LangChain フレームワークでは、RAGなどのパターンを効率的に構築できるさまざまなサービスやツールと統合されています。以下のツールを使用します。\n",
    "\n",
    "- **LLM (Large Language Model)**: Amazon Bedrock から入手可能な Anthropic Claude V1\n",
    "\n",
    "  このモデルを使って文書の塊を理解し、わかりやすい方法で答えを出します。\n",
    "- **Embeddings Model**: Amazon Bedrock から入手可能な Amazon Titan Embeddings\n",
    "\n",
    "  このモデルを使用して、テキスト文書の数値表現を生成します。\n",
    "- **Document Loader**: LangChain から入手可能な PDF Loader\n",
    "\n",
    "  これはソースからドキュメントをロードできるローダーです。このノートブックでは、ローカルパスからサンプルファイルをロードしています。これを、企業の内部システムから文書を読み込むためのローダーに簡単に置き換えることができます。\n",
    "\n",
    "- **Vector Store**: LangChain から入手可能な FAISS \n",
    "\n",
    "  このノートブックでは、このメモリ内ベクトルストアを使用して、埋め込み (Embeddings) とドキュメントの両方を保存しています。エンタープライズ環境では、これを AWS OpenSearch、PgVector 搭載の RDS Postgres、ChromaDB、Pinecone、Weaviate などの永続的なストアに置き換えることができます。\n",
    "- **Index**: ベクトルインデックス\n",
    "\n",
    "  The index helps to compare the input embedding and the document embeddings to find relevant document\n",
    "- **Wrapper**: Index、Vector Store、Embeddings Model、LLMをラップして、ロジックをユーザーから抽象化します。\n",
    "\n",
    "この [ノートブック](https://www.pinecone.io/learn/series/langchain/langchain-retrieval-augmentation/) とこの [ノートブック](01_qa_w_rag_claude.ipynb) のアイデアの助けを借りて構築されました"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## セットアップ\n",
    "\n",
    "このノートブックの残りの部分を実行する前に、以下のセルを実行して (必要なライブラリがインストールされていることを確認し) Bedrockに接続する必要があります。\n",
    "\n",
    "For more details on how the setup works and ⚠️ **whether you might need to make any changes**, refer to the [Bedrock boto3 setup notebook](../00_Intro/bedrock_boto3_setup.ipynb) notebook.\n",
    "セットアップの仕組みと ⚠️ **変更が必要かどうかについての詳細**は、[Bedrock boto3 セットアップノートブック](../00_Intro/Bedrock_boto3_Setup.ipynb) を参照してください。\n",
    "\n",
    "このノートブックでは、いくつかの追加の依存関係も必要になります。\n",
    "\n",
    "- [Pinecone](http://pinecone.io), ベクター埋め込みを保存\n",
    "- [PyPDF](https://pypi.org/project/pypdf/), PDF ファイルの処理用\n",
    "\n",
    "### Pinecone 前提条件\n",
    "- app.pinecone.io の Pinecone API キーを環境変数 PINECONE_API_KEY として設定\n",
    "- Pinecone 環境 - コンソールで API キーの横を検索し、そのキーを環境変数 PINECONE_ENVIRONMENT として設定します"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install --no-build-isolation --force-reinstall \\\n",
    "    \"boto3>=1.28.57\" \\\n",
    "    \"awscli>=1.29.57\" \\\n",
    "    \"botocore>=1.31.57\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install -U \"faiss-cpu>=1.7,<2\" langchain==0.0.309 \"pypdf>=3.8,<4\" \\\n",
    "    pinecone-client \\\n",
    "    apache-beam \\\n",
    "    datasets \\\n",
    "    tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import boto3\n",
    "\n",
    "module_path = \"..\"\n",
    "sys.path.append(os.path.abspath(module_path))\n",
    "from utils import bedrock, print_ww\n",
    "\n",
    "\n",
    "# ---- ⚠️ 必要に応じて AWS 設定に関する以下のコードのコメントを解除、編集してください ⚠️ ----\n",
    "\n",
    "# os.environ[\"AWS_DEFAULT_REGION\"] = \"<REGION_NAME>\"  # E.g. \"us-east-1\"\n",
    "# os.environ[\"AWS_PROFILE\"] = \"<YOUR_PROFILE>\"\n",
    "# os.environ[\"BEDROCK_ASSUME_ROLE\"] = \"<YOUR_ROLE_ARN>\"  # E.g. \"arn:aws:...\"\n",
    "\n",
    "boto3_bedrock = bedrock.get_bedrock_client(\n",
    "    assumed_role=os.environ.get(\"BEDROCK_ASSUME_ROLE\", None),\n",
    "    region=os.environ.get(\"AWS_DEFAULT_REGION\", None)\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Langchainの設定\n",
    "\n",
    "まず、LLM と埋め込みモデルをインスタンス化します。ここでは、テキスト生成には Anthropic Claude を使用し、テキストの埋め込みには Amazon Titan を使用しています。\n",
    "\n",
    "注:Bedrock では他のモデルも選択できます。`model_id`を以下のように置き換えてモデルを変更することができます。\n",
    "\n",
    "`llm = Bedrock(model_id=\"amazon.titan-tg1-large\")`\n",
    "\n",
    "利用可能な `model_id`:\n",
    "\n",
    "- `amazon.titan-tg1-large`\n",
    "- `ai21.j2-grande-instruct`\n",
    "- `ai21.j2-jumbo-instruct`\n",
    "- `anthropic.claude-instant-v1`\n",
    "- `anthropic.claude-v1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Titan Embeddings モデルを使用して埋め込みを生成します。\n",
    "from langchain.embeddings import BedrockEmbeddings\n",
    "from langchain.llms.bedrock import Bedrock\n",
    "\n",
    "# - Anthropic モデルの作成\n",
    "llm = Bedrock(\n",
    "    model_id=\"anthropic.claude-v1\", client=boto3_bedrock, model_kwargs={\"max_tokens_to_sample\": 200}\n",
    ")\n",
    "bedrock_embeddings = BedrockEmbeddings(client=boto3_bedrock)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データの準備\n",
    "まず、いくつかのファイルをダウンロードして、ドキュメントストアを構築しましょう。この例では、[ここ](https://www.irs.gov/publications) にある公開の IRS 文書を使用します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from urllib.request import urlretrieve\n",
    "\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "files = [\n",
    "    \"https://www.irs.gov/pub/irs-pdf/p1544.pdf\",\n",
    "    \"https://www.irs.gov/pub/irs-pdf/p15.pdf\",\n",
    "    \"https://www.irs.gov/pub/irs-pdf/p1212.pdf\",\n",
    "]\n",
    "for url in files:\n",
    "    file_path = os.path.join(\"data\", url.rpartition(\"/\")[2])\n",
    "    urlretrieve(url, file_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ダウンロード後、[LangChainで利用可能なPyPDFのディレクトリローダー](https://python.langchain.com/en/latest/reference/modules/document_loaders.html)の助けを借りてドキュメントをロードし、小さなチャンクに分割することができます。\n",
    "\n",
    "注：検索された文書/テキストは、質問に答えるのに十分な情報を含む大きさでなければなりませんが、LLMプロンプトに収まる大きさであれば十分です。また、埋め込みモデルでは、入力トークンの長さを512トークンまでに制限しています。このユースケースのために、私たちは[RecursiveCharacterTextSplitter](https://python.langchain.com/en/latest/modules/indexes/text_splitters/examples/recursive_text_splitter.html)を使用して、100文字のオーバーラップで約1000文字のチャンクを作成しています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import PyPDFLoader, PyPDFDirectoryLoader\n",
    "\n",
    "loader = PyPDFDirectoryLoader(\"./data/\")\n",
    "\n",
    "documents = loader.load()\n",
    "# - 私たちのテストでは、文字分割はこのPDFデータセットの方がうまく機能します\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # 見せるために、本当に小さいチャンクサイズを設定してください。\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=100,\n",
    ")\n",
    "docs = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "avg_doc_length = lambda documents: sum([len(doc.page_content) for doc in documents]) // len(\n",
    "    documents\n",
    ")\n",
    "avg_char_count_pre = avg_doc_length(documents)\n",
    "avg_char_count_post = avg_doc_length(docs)\n",
    "print(f\"Average length among {len(documents)} documents loaded is {avg_char_count_pre} characters.\")\n",
    "print(f\"After the split we have {len(docs)} documents more than the original {len(documents)}.\")\n",
    "print(\n",
    "    f\"Average length among {len(docs)} documents (after split) is {avg_char_count_post} characters.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    \n",
    "    sample_embedding = np.array(bedrock_embeddings.embed_query(docs[0].page_content))\n",
    "    modelId = bedrock_embeddings.model_id\n",
    "    print(\"Embedding model Id :\", modelId)\n",
    "    print(\"Sample embedding of a document chunk: \", sample_embedding)\n",
    "    print(\"Size of the embedding: \", sample_embedding.shape)\n",
    "\n",
    "except ValueError as error:\n",
    "    if  \"AccessDeniedException\" in str(error):\n",
    "        print(f\"\\x1b[41m{error}\\\n",
    "        \\nTo troubeshoot this issue please refer to the following resources.\\\n",
    "         \\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/troubleshoot_access-denied.html\\\n",
    "         \\nhttps://docs.aws.amazon.com/bedrock/latest/userguide/security-iam.html\\x1b[0m\\n\")      \n",
    "        class StopExecution(ValueError):\n",
    "            def _render_traceback_(self):\n",
    "                pass\n",
    "        raise StopExecution        \n",
    "    else:\n",
    "        raise error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同様のパターン埋め込みをコーパス全体に対して生成し、ベクトルストアに保存することができます。\n",
    "\n",
    "これは、[LangChain](https://python.langchain.com/en/latest/modules/indexes/vectorstores/examples/faiss.html)内の[Pinecone](https://python.langchain.com/docs/integrations/vectorstores/pinecone)実装を使うことで、簡単に実現できます。この実装は、埋め込みモデルとドキュメントを入力として受け取り、ベクトルストア全体を作成します。Index Wrapperを使うことで、プロンプトの作成、クエリの埋め込みデータの取得、関連するドキュメントのサンプリング、LLMの呼び出しといった重い作業のほとんどを抽象化することができます。[VectorStoreIndexWrapper](https://python.langchain.com/en/latest/modules/indexes/getting_started.html#one-line-index-creation)がその手助けをしてくれます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pinecone\n",
    "import time\n",
    "import os\n",
    "\n",
    "# app.pinecone.io からPinecone API キーを追加\n",
    "api_key = os.environ.get(\"PINECONE_API_KEY\") or \"YOUR_API_KEY\"\n",
    "# Pinecone 環境の設定-コンソールの API キーの横を検索\n",
    "env = os.environ.get(\"PINECONE_ENVIRONMENT\") or \"YOUR_ENV\"\n",
    "\n",
    "pinecone.init(api_key=api_key, environment=env)\n",
    "\n",
    "\n",
    "if index_name in pinecone.list_indexes():\n",
    "    pinecone.delete_index(index_name)\n",
    "\n",
    "pinecone.create_index(name=index_name, dimension=sample_embedding.shape[0], metric=\"dotproduct\")\n",
    "# インデックスの初期化が完了するまで待つ\n",
    "while not pinecone.describe_index(index_name).status[\"ready\"]:\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "index = pinecone.Index(index_name)\n",
    "index.describe_index_stats()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "**⚠️⚠️⚠️ 注:次のセルの実行には数分かかる場合があります ⚠️⚠️⚠️**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from uuid import uuid4\n",
    "from langchain.vectorstores import Pinecone\n",
    "\n",
    "batch_limit = 50\n",
    "\n",
    "texts = []\n",
    "metadatas = []\n",
    "\n",
    "for i, record in enumerate(tqdm(data)):\n",
    "    # 最初にこのレコードのメタデータフィールドを取得\n",
    "    metadata = {\"wiki-id\": str(record[\"id\"]), \"source\": record[\"url\"], \"title\": record[\"title\"]}\n",
    "    # 今度はレコードテキストからチャンクを作成します\n",
    "    record_texts = text_splitter.split_text(record[\"text\"])\n",
    "    # チャンクごとに個別のメタデータ辞書を作成\n",
    "    record_metadatas = [\n",
    "        {\"chunk\": j, \"text\": text, **metadata} for j, text in enumerate(record_texts)\n",
    "    ]\n",
    "    # これらを現在のバッチに追加\n",
    "    texts.extend(record_texts)\n",
    "    metadatas.extend(record_metadatas)\n",
    "    # batch_limit に達したら、テキストを追加できます\n",
    "    if len(texts) >= batch_limit:\n",
    "        ids = [str(uuid4()) for _ in range(len(texts))]\n",
    "        # embeds = embed.embed_documents(texts)\n",
    "        embeds = np.array([np.array(bedrock_embeddings.embed_query(text)) for text in texts])\n",
    "        index.upsert(vectors=zip(ids, embeds.tolist(), metadatas))\n",
    "        texts = []\n",
    "        metadatas = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "index.describe_index_stats()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangChain ベクトルストアとクエリ\n",
    "\n",
    "インデックスは LangChain とは独立して構築されます。これは単純なプロセスであり、Pinecone クライアントで直接行うほうが速いからです。ただし、LangChain に戻ろうとしているので、LangChain ライブラリーを介してインデックスに再接続する必要があります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Pinecone\n",
    "\n",
    "text_field = \"text\"\n",
    "\n",
    "# langchainの通常のインデックスに戻す\n",
    "index = pinecone.Index(index_name)\n",
    "\n",
    "vectorstore = Pinecone(index, bedrock_embeddings.embed_query, text_field)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 類似性検索メソッドを使用すると、LLM がレスポンスを生成しなくても、直接クエリを実行してテキストのチャンクを返すことができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = \"Is it possible that I get sentenced to jail due to failure in filings?\"\n",
    "\n",
    "vectorstore.similarity_search(query, k=3)  # 検索クエリ # 最も関連性の高い 3 つのドキュメントを返す"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### これらはすべて関連する結果であり、システムの検索コンポーネントが機能していることを示しています。次のステップは、取得したコンテキストで提供された情報を使用して質問に生成的に回答する LLM を追加することです。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 生成型質問応答 (Generative Question Answering)\n",
    "\n",
    "生成型質問応答 (GQA) では、質問をClaude-2に渡しますが、ナレッジベースから返された情報に基づいて回答するように指示します。LangChain では RetrievalQA チェーンを使ってこれを簡単に行うことができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=vectorstore.as_retriever())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### これを先ほどのクエリで試してみましょう:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "qa.run(query)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 今回のレスポンスは、ベクトルデータベースから取得した情報に基づいてgpt-3.5-turbo LLMが生成したものです。\n",
    "\n",
    "私たちはまだ、モデルによる説得力のある間違ったハルシネーションから完全に守られているわけではありません。しかし、私たちは提供される答えに対する信頼を高めるためにもっとできることがあります。\n",
    "\n",
    "そのための効果的な方法は、回答に引用を加えることで、ユーザーがその情報の出所を確認できるようにすることです。これは、RetrievalQAWithSourcesChainと呼ばれるRetrievalQAチェーンの少し異なるバージョンを使用して行うことができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "\n",
    "qa_with_sources = RetrievalQAWithSourcesChain.from_chain_type(\n",
    "    llm=llm, chain_type=\"stuff\", retriever=vectorstore.as_retriever()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "qa_with_sources(query)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### これで、質問への回答ができました。また、LLMが使用しているこの情報の出所も記載しました。\n",
    "\n",
    "#### ベクトルデータベースをナレッジベースとして使用して、ソース知識に基づいて大規模言語モデルをベースにする方法を学びました。これを利用することで、すべての回答に引用を提供することで、LLMの回答の正確性を高め、情報源の知識を最新の状態に保ち、システムへの信頼性を高めることができます。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このクエリの埋め込みを使用して、関連するドキュメントを取得できます。\n",
    "クエリが埋め込みとして表示されるようになったので、クエリをデータストアと類似検索して、最も関連性の高い情報を得ることができます。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### カスタマイズ可能なオプション\n",
    "上記のシナリオでは、質問に対する文脈を考慮した回答を得るための、迅速で簡単な方法を探りました。次に、[RetrievalQA](https://python.langchain.com/en/latest/modules/chains/index_examples/vector_db_qa.html)の助けを借りて、よりカスタマイズ可能なオプションを見てみましょう。ここでは、`chain_type`パラメータを使用して、取得したドキュメントをプロンプトに追加する方法をカスタマイズすることができます。また、何件の関連文書を取得するかを制御したい場合は、以下のセルの `k` パラメータを変更すると、異なる出力を見ることができます。多くのシナリオでは、LLM が答えを生成するために使用したソース文書がどれであるかを知りたい場合があります。また、`RetrievalQA`では、モデルに固有のカスタム[プロンプトテンプレート](https://python.langchain.com/en/latest/modules/prompts/prompt_templates/getting_started.html) を指定することができます。\n",
    "\n",
    "注: この例では、Amazon BedrockのLLMとしてAnthropic Claudeを使用しています。この特定のモデルは、入力が`Human:`の下で提供され、モデルが`Assistant:`の後に出力を生成するように要求された場合に、最高のパフォーマンスを発揮します。下のセルでは、LLMがコンテキストから外れた回答をしないように、プロンプトをコントロールする方法の例を示しています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = \"\"\"Human: Use the following pieces of context to provide a concise answer to the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Assistant:\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectorstore.as_retriever(),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": PROMPT},\n",
    ")\n",
    "query = \"Is it possible that I get sentenced to jail due to failure in filings?\"\n",
    "result = qa({\"query\": query})\n",
    "print_ww(result[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result[\"source_documents\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 結論\n",
    "検索拡張生成 (RAG) に関するこのモジュールの完了、おめでとうございます！これは、大規模言語モデルのパワーと検索手法の精度を組み合わせた重要なテクニックです。関連する検索例で生成を補強することで、得られた回答はより首尾一貫し、一貫性があり、根拠があるものになります。この革新的なアプローチを学んだことを誇りに思うべきです。あなたが得た知識は、創造的で魅力的な言語生成システムを構築するのに非常に役立つと確信しています。よく頑張りました！\n",
    "\n",
    "上記のRAGベースの質問応答の実装では、Amazon Bedrock と LangChain の統合を使って、以下のコンセプトとその実装方法を探りました。\n",
    "\n",
    "- ベクトルストアを作成するために、ドキュメントをロードし、埋め込みを生成する\n",
    "- 質問に対するドキュメントの取得\n",
    "- LLMへの入力となるプロンプトを準備する\n",
    "- 人間にとって使いやすい方法で答えを提示する\n",
    "- すべての回答に引用を提供することで、ソース知識を最新の状態に保ち、システムの信頼性を向上させる\n",
    "\n",
    "### 課題\n",
    "- 様々なベクトルストアを試す\n",
    "- Amazon Bedrockで利用可能な様々なモデルを活用し、別の出力を確認する\n",
    "- 埋め込みやドキュメントチャンクの永続的な保存などのオプションを検討する\n",
    "- エンタープライズデータストアとの統合\n",
    "\n",
    "# ありがとうございました。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
