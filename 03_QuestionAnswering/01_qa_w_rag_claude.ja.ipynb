{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChainを使用したAmazon Bedrockによる検索拡張質問と回答\n",
    "\n",
    "> *このノートブックは SageMaker Studio の **`Data Science 3.0`** カーネルをご利用ください*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### コンテキスト\n",
    "前回、モデルがタイヤの交換方法を回答することを確認しましたが、我々は関連データを手動で提供し、コンテキストを自分たちで提供する必要がありました。私たちは、Bedrockで利用可能なモデルを活用し、トレーニング中に学習した知識に基づいて質問し、手動でコンテキストを提供するアプローチを探りました。このアプローチは短いドキュメントやシングルトンのアプリケーションでは機能しますが、モデルに送信されるプロンプトにすべてが収まらない大規模なエンタープライズドキュメントが存在する可能性があるエンタープライズレベルの質問応答には拡張することができません。\n",
    "\n",
    "### パターン\n",
    "Retreival Augmented Generation (RAG) と呼ばれるアーキテクチャを実装することで、このプロセスを改善することができます。RAG は言語モデルの外部でデータを検索し（ノンパラメトリック）、関連する検索データをコンテキストに追加することでプロンプトを補強します。\n",
    "\n",
    "このノートブックでは、ユーザーの質問に対する答えを提供するために文書を検索し、活用する質問応答のパターンへのアプローチ方法を説明します。\n",
    "\n",
    "### チャレンジ\n",
    "- トークン制限を超える大きな文書を管理する方法\n",
    "- 質問に関連する文書を見つける方法\n",
    "\n",
    "### 提案\n",
    "上記の課題に対して、このノートブックは以下の戦略を提案します。\n",
    "\n",
    "#### ドキュメントの準備\n",
    "![Embeddings](./images/Embeddings_lang.png)\n",
    "\n",
    "質問に答える前に、ドキュメントを処理し、ドキュメントストアインデックスに格納する必要があります。\n",
    "- ドキュメントを読み込む\n",
    "- 処理して小さなチャンクに分割する\n",
    "- Amazon Bedrock Titan Embeddings モデルを使用して、各チャンクの数値ベクトル表現を作成する\n",
    "- チャンクと対応する埋め込みを使用してインデックスを作成する\n",
    "\n",
    "### 質問をする\n",
    "![Question](./images/Chatbot_lang.png)\n",
    "\n",
    "文書インデックスが準備できたら、質問を行い、質問に基づいて関連文書を検索することができるようになります。以下のステップが実行されます。\n",
    "- 入力された質問の埋め込みを作成する\n",
    "- 質問の Embedding をインデックスの埋め込みと比較する\n",
    "- (上位 N 件の) 関連するドキュメントのチャンクを取得する\n",
    "- これらのチャンクをプロンプトのコンテキストの一部として追加する\n",
    "- Amazon Bedrock のモデルにプロンプトを送信する\n",
    "- 取得した文書に基づいて、文脈に沿った回答を得る"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ユースケース\n",
    "#### データセット\n",
    "このアーキテクチャパターンを説明するために、IRS のドキュメントを使用します。これらの文書では、以下のようなトピックが説明されています：\n",
    "- オリジネーション時の元本割引（OID）操作\n",
    "- IRS への 10,000 ドルを超える現金支払いの報告\n",
    "- 雇用者向けの税務ガイド\n",
    "\n",
    "#### ペルソナ\n",
    "IRS の仕組みや、ある行動がどのような意味を持つのか、また持たないのかを理解していない素人を想定してみましょう。\n",
    "モデルは文書に基づいて簡単な言葉で答えようとします。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 実装\n",
    "\n",
    "RAG のアプローチに沿うために、このノートブックは LangChain フレームワークを使用しています。LangChain フレームワークは、RAG のようなパターンを効率的に構築するための様々なサービスやツールと統合されています。以下のツールを使います：\n",
    "\n",
    "- **LLM (大規模言語モデル)**: Amazon Bedrock で使用可能な Anthropic Claude V1 \n",
    "\n",
    "  このモデルは、ドキュメントのチャンクを理解し、人間が理解しやすい方法で答えを提供するために使用されます。\n",
    "- **埋め込みモデル**: Amazon Bedrock で使用可能な Amazon Titan Embeddings\n",
    "\n",
    "  このモデルは、テキスト文書の数値表現を生成するために使用されます。\n",
    "- **ドキュメントローダー**: LangChain を通して利用可能な PDF Loader\n",
    "\n",
    "  このノートブックでは、ローカルパスからサンプルファイルをロードするために、このドキュメントローダーを使用します。これは、企業の内部システムからドキュメントをロードするローダーに簡単に置き換えることができます。\n",
    "\n",
    "- **ベクトルストア**: LangChain を通して利用可能な FAISS\n",
    "\n",
    "  このノートブックでは、埋め込みとドキュメントの両方を保存するために、このインメモリのベクトルストアを使用しています。エンタープライズのユースケースでは、AWS OpenSearch、RDS for PostgreSQL with pgVector、ChromaDB、Pinecone、Weaviate のような永続的なストアに置き換えることができます。\n",
    "- **インデックス**: VectorIndex\n",
    "\n",
    "  このインデックスは、関連するドキュメントを見つけるために、入力の埋め込みとドキュメントの埋め込みを比較する場合に役立ちます。\n",
    "- **ラッパー**: インデックス、ベクトルストア、埋め込みモデル、LLM をラップし、ロジックを抽象化します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## セットアップ\n",
    "\n",
    "このノートブックの残りの部分を実行する前に、以下のセルを実行して (必要なライブラリがインストールされていることを確認し) Bedrock に接続する必要があります。\n",
    "\n",
    "セットアップの仕組みと ⚠️ **whether you might need to make any changes** についての詳細は、[Bedrock boto3 setup ノートブック](../00_Intro/bedrock_boto3_setup.ja.ipynb)  を参照してください。\n",
    "\n",
    "このノートブックでは、追加の依存関係が必要になります：\n",
    "\n",
    "- [FAISS](https://github.com/facebookresearch/faiss) ... 埋め込みベクトルを保存するため\n",
    "- [PyPDF](https://pypi.org/project/pypdf/) ... PDF ファイルを操作するため"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 事前にリポジトリルートから `download-dependencies.sh` を実行済みであることを確認してください!\n",
    "%pip install --no-build-isolation --force-reinstall \\\n",
    "    ../dependencies/awscli-*-py3-none-any.whl \\\n",
    "    ../dependencies/boto3-*-py3-none-any.whl \\\n",
    "    ../dependencies/botocore-*-py3-none-any.whl\n",
    "\n",
    "%pip install --quiet \"faiss-cpu>=1.7,<2\" langchain==0.0.249 \"pypdf>=3.8,<4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import boto3\n",
    "\n",
    "module_path = \"..\"\n",
    "sys.path.append(os.path.abspath(module_path))\n",
    "from utils import bedrock, print_ww\n",
    "\n",
    "\n",
    "# ---- ⚠️ 必要に応じて AWS 設定に関する以下のコードのコメントを解除、編集してください ⚠️ ----\n",
    "\n",
    "# os.environ[\"AWS_DEFAULT_REGION\"] = \"<REGION_NAME>\"  # E.g. \"us-east-1\"\n",
    "# os.environ[\"AWS_PROFILE\"] = \"<YOUR_PROFILE>\"\n",
    "# os.environ[\"BEDROCK_ASSUME_ROLE\"] = \"<YOUR_ROLE_ARN>\"  # E.g. \"arn:aws:...\"\n",
    "# os.environ[\"BEDROCK_ENDPOINT_URL\"] = \"<YOUR_ENDPOINT_URL>\"  # E.g. \"https://...\"\n",
    "\n",
    "\n",
    "boto3_bedrock = bedrock.get_bedrock_client(\n",
    "    assumed_role=os.environ.get(\"BEDROCK_ASSUME_ROLE\", None),\n",
    "    endpoint_url=os.environ.get(\"BEDROCK_ENDPOINT_URL\", None),\n",
    "    region=os.environ.get(\"AWS_DEFAULT_REGION\", None),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangChain の設定\n",
    "\n",
    "\n",
    "まずは LLM と埋め込み (Embedding) モデルのインスタンス化から始めます。ここでは、テキスト生成に Anthropic Claude を、テキスト埋め込みに Amazon Titan を使っています。\n",
    "\n",
    "注意: Bedrock で用いることができる他のモデルを選択することも可能です。 `model_id` を以下のように置き換えることでモデルを変更可能です。\n",
    "\n",
    "`llm = Bedrock(model_id=\"amazon.titan-tg1-large\")`\n",
    "\n",
    "使用可能な model ID:\n",
    "\n",
    "- `amazon.titan-tg1-large`\n",
    "- `ai21.j2-grande-instruct`\n",
    "- `ai21.j2-jumbo-instruct`\n",
    "- `anthropic.claude-instant-v1`\n",
    "- `anthropic.claude-v2`\n",
    "\n",
    "テキスト埋め込みについても同様です:\n",
    "\n",
    "`bedrock_embeddings = BedrockEmbeddings(model_id=\"amazon.titan-embed-g1-text-02\")`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 埋め込みの生成には、 Titan Embeddings モデルを使用します。\n",
    "from langchain.embeddings import BedrockEmbeddings\n",
    "from langchain.llms.bedrock import Bedrock\n",
    "\n",
    "# Anthropic のモデルを作成します\n",
    "llm = Bedrock(model_id=\"anthropic.claude-v2\", client=boto3_bedrock, model_kwargs={'max_tokens_to_sample':200})\n",
    "bedrock_embeddings = BedrockEmbeddings(model_id=\"amazon.titan-embed-g1-text-02\", client=boto3_bedrock)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データ準備\n",
    "はじめに、ドキュメントストアを作成するためにいくつかのファイルをダウンロードしましょう。今回の例では公開されている IRS のドキュメントを[こちら](https://www.irs.gov/publications)から使用します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlretrieve\n",
    "\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "files = [\n",
    "    \"https://www.irs.gov/pub/irs-pdf/p1544.pdf\",\n",
    "    \"https://www.irs.gov/pub/irs-pdf/p15.pdf\",\n",
    "    \"https://www.irs.gov/pub/irs-pdf/p1212.pdf\",\n",
    "]\n",
    "for url in files:\n",
    "    file_path = os.path.join(\"data\", url.rpartition(\"/\")[2])\n",
    "    urlretrieve(url, file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ダウンロードののち、[DirectoryLoader from PyPDF available under LangChain](https://python.langchain.com/en/latest/reference/modules/document_loaders.html)を参照しながらドキュメントをロードし、より小さいチャンクに分割することができます。\n",
    "\n",
    "注意: 検索された文書/テキストは、質問に答えるのに十分な情報を含む大きさでなければなりませんが、さらに LLM プロンプトに収まる大きさでなければなりません。また、埋め込み (Embedding) モデルでは、入力トークンの長さを 8192 トークンまでに制限されており、これはおよそ 32,000 文字に相当します。このユースケースのために、私たちは[RecursiveCharacterTextSplitter](https://python.langchain.com/en/latest/modules/indexes/text_splitters/examples/recursive_text_splitter.html)を使って 100 文字のオーバーラップで約 1000 文字のチャンクを作成しています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import PyPDFLoader, PyPDFDirectoryLoader\n",
    "\n",
    "loader = PyPDFDirectoryLoader(\"./data/\")\n",
    "\n",
    "documents = loader.load()\n",
    "# - 私たちのテストでは、文字分割はこの PDF データセットでより良く機能します。\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # 表示するために、非常に小さいチャンクを指定してください。\n",
    "    chunk_size = 1000,\n",
    "    chunk_overlap  = 100,\n",
    ")\n",
    "docs = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_doc_length = lambda documents: sum([len(doc.page_content) for doc in documents])//len(documents)\n",
    "avg_char_count_pre = avg_doc_length(documents)\n",
    "avg_char_count_post = avg_doc_length(docs)\n",
    "print(f'Average length among {len(documents)} documents loaded is {avg_char_count_pre} characters.')\n",
    "print(f'After the split we have {len(docs)} documents more than the original {len(documents)}.')\n",
    "print(f'Average length among {len(docs)} documents (after split) is {avg_char_count_post} characters.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 つのPDFドキュメントがあり、それらは約 500 個の小さなチャンクに分割されています。\n",
    "\n",
    "これで、わたしたちはこれらのチャンクについて、サンプルの埋め込みがどのように見えるのかを確認することができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_embedding = np.array(bedrock_embeddings.embed_query(docs[0].page_content))\n",
    "print(\"Sample embedding of a document chunk: \", sample_embedding)\n",
    "print(\"Size of the embedding: \", sample_embedding.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同様のパターンで、コーパス全体の埋め込みを生成し、ベクトルストアに保存することができます。\n",
    "\n",
    "これは [LangChain](https://python.langchain.com/en/latest/modules/indexes/vectorstores/examples/faiss.html) 内の [FAISS](https://github.com/facebookresearch/faiss) 実装を使用して簡単に実行することができます。埋め込み (Embedding) モデルとドキュメントを入力してベクトルストア全体を作成します。\n",
    "Index Wrapper を使うと、プロンプトの作成、クエリの埋め込みの取得、関連ドキュメントのサンプリング、LLM の呼び出しといった面倒な作業のほとんどを抽象化することができます。[VectorStoreIndexWrapper](https://python.langchain.com/en/latest/modules/indexes/getting_started.html#one-line-index-creation)  が有用です。\n",
    "\n",
    "**⚠️⚠️⚠️ 注意: 次のセルの実行には数分かかる場合があります ⚠️⚠️⚠️**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain.indexes.vectorstore import VectorStoreIndexWrapper\n",
    "\n",
    "vectorstore_faiss = FAISS.from_documents(\n",
    "    docs,\n",
    "    bedrock_embeddings,\n",
    ")\n",
    "\n",
    "wrapper_store_faiss = VectorStoreIndexWrapper(vectorstore=vectorstore_faiss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 質問応答\n",
    "\n",
    "現在、ベクトルストアが用意できているため、質問を始めることができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Is it possible that I get sentenced to jail due to failure in filings?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最初のステップは、ドキュメントと比較できるようにクエリの埋め込みを作成することです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_embedding = vectorstore_faiss.embedding_function(query)\n",
    "np.array(query_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このクエリの埋め込みを使用して、関連するドキュメントを取得できます。\n",
    "クエリが埋め込みとして表現されるようになった今、クエリをデータストアに対して類似検索することで、最も関連性の高い情報が得られます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_documents = vectorstore_faiss.similarity_search_by_vector(query_embedding)\n",
    "print(f'{len(relevant_documents)} documents are fetched which are relevant to the query.')\n",
    "print('----')\n",
    "for i, rel_doc in enumerate(relevant_documents):\n",
    "    print_ww(f'## Document {i+1}: {rel_doc.page_content}.......')\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "関連ドキュメントが揃ったので、今度は LLM を使用してこれらのドキュメントに基づいて回答を生成します。 \n",
    "\n",
    "最初のプロンプトと、類似検索の結果に基づいて検索された関連ドキュメントを取得します。次に、これらを組み合わせてプロンプトを作成し、モデルにフィードバックして結果を取得します。\n",
    "\n",
    "LangChain は、これらを容易に行うための抽象化を提供します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 簡単な方法\n",
    "LangChain が提供するラッパーを使用することもできます。このラッパーはベクトルストアをラップし、LLM の入力を受け取ります。\n",
    "このラッパーは、バックグラウンドで以下のステップを実行します。\n",
    "- 質問を入力する\n",
    "- 質問の Embedding を作成する\n",
    "- 関連文書を取得する\n",
    "- 書類と質問をプロンプトに詰め込む\n",
    "- プロンプトを使用してモデルを起動し、人間の理解しやすい表現で回答を生成する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "prompt_template = \"\"\"Human: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Assistant:\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "answer = wrapper_store_faiss.query(question=query, llm=llm, chain_type_kwargs={\"prompt\": PROMPT})\n",
    "print_ww(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "違う質問をしてみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_2 = \"What is the difference between market discount and qualified stated interest\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "prompt_template = \"\"\"Human: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Assistant:\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "answer_2 = wrapper_store_faiss.query(question=query_2, llm=llm, chain_type_kwargs={\"prompt\": PROMPT})\n",
    "print_ww(answer_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### カスタマイズ可能なオプション\n",
    "\n",
    "上記のシナリオでは、質問に対するコンテキストに応じた回答をすばやく簡単に取得する方法を検討しました。それでは、[RetrievalQA](https://python.langchain.com/en/latest/modules/chains/index_examples/vector_db_qa.html) を用いて、よりカスタマイズ可能なオプションを見てみましょう。これを用いると、`chain_type` パラメータを使用して、取得したドキュメントをプロンプトに追加する方法をカスタマイズできます。 また、取得する関連ドキュメントの数を制御したい場合は、下のセルの `k` パラメータを変更して異なる出力を確認してみてください。LLM が回答の生成に使用したソースドキュメントを知りたい場合も多いでしょう。その場合は、LLM プロンプトのコンテキストに追加されたドキュメントを返す `return_source_documents` を使用してそれらのドキュメントを Output として取得できます。`RetrievalQA` ではさらに、モデル固有にカスタムされた [プロンプトテンプレート](https://python.langchain.com/en/latest/modules/prompts/prompt_templates/getting_started.html) を使用することもできます。\n",
    "\n",
    "注意:この例では、Amazon Bedrock の LLM として Anthropic Claude を使用しています。この特定のモデルは、入力が `Human:` の後に提供され、モデルが `Assistant:` の後に出力を生成するように要求された場合に最も効果的です。[詳細はこちらをご参照ください](https://docs.anthropic.com/claude/docs/human-and-assistant-formatting) 下のセルでは、LLMがコンテキストを超えた回答をしないように、プロンプトを制御する方法の一例を見ることができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "\n",
    "Human: Use the following pieces of context to provide a concise answer to the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "<context>\n",
    "{context}\n",
    "</context\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Assistant:\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectorstore_faiss.as_retriever(\n",
    "        search_type=\"similarity\", search_kwargs={\"k\": 3}\n",
    "    ),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": PROMPT}\n",
    ")\n",
    "query = \"Is it possible that I get sentenced to jail due to failure in filings?\"\n",
    "result = qa({\"query\": query})\n",
    "print_ww(result['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result['source_documents']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## まとめ\n",
    "あなたは検索拡張生成 (RAG) に関するモジュールを完了しました！これは、大規模言語モデルの能力と検索を組み合わせた重要な手法です。関連する検索例で生成を増やすことで、受け取った応答はより一貫性があり、かつ根拠のあるものになります。あなたはこの革新的なアプローチを学んだことを誇りに思うはずです。あなたが得た知識は、創造的で魅力的な言語生成システムを構築する際にかならず役立つでしょう。\n",
    "\n",
    "上記の RAG ベースの質問応答機能の実装では、以下の概念と、Amazon Bedrock と LangChain の統合を使用してそれらを実装する方法を検討しました。\n",
    "\n",
    "- ドキュメントのロードと埋め込みの生成によるベクトルストアの作成する\n",
    "- 質問のための文書を取得する\n",
    "- LLM への入力として送られるプロンプトの準備する\n",
    "- 人間の理解しやすい表現で回答を提示する\n",
    "\n",
    "### 発展要素\n",
    "- さまざまなベクトルストアを試してみる\n",
    "- Amazon Bedrock で入手可能なさまざまなモデルを活用して、出力を確認する\n",
    "- 埋め込みやドキュメントチャンクの永続ストレージなどのオプションを検討する\n",
    "- エンタープライズデータストアとの統合\n",
    "\n",
    "# ありがとうございました!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
