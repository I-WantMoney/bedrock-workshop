# ラボ 3-質問への応答

## はじめに

質問応答（QA）は、自然言語で提起された事実に基づく質問に対する回答を抽出することを含む重要なタスクです。通常、QA システムは、構造化データまたは非構造化データを含むナレッジベースに対するクエリを処理し、正確な情報を含む応答を生成します。特に企業のユースケースでは、有用で信頼性が高く、信頼できる質問応答システムを開発するには、高い精度を確保することが鍵となります。

Amazon Titan、Anthropic Claude、AI21 Jurassic 2などのジェネレーティブAIモデルは、確率分布を使用して質問に対する回答を生成します。これらのモデルは膨大な量のテキストデータに基づいてトレーニングされているため、シーケンス内で次に何が起こるか、特定の単語の後にどの単語が続くかを予測できます。ただし、データには常にある程度の不確実性があるため、これらのモデルではすべての質問に対して正確または決定論的な答えを出すことはできません。

企業は、ドメイン固有の独自データを照会し、その情報を使用して質問に答える必要があります。より一般的には、モデルのトレーニングを受けていないデータにも対応する必要があります。

## パターン

これらのラボでは、次の 2 つの QA パターンを調べます。

1.  最初に質問がモデルに送られ、そこで修正なしで基本モデルに基づいて回答が得られます。
    これには課題があります。
    アウトプットは一般的な世界の情報に限ったもので、顧客固有のビジネスに固有のものではなく、情報源もありません。

    ![Q\&A](./images/51-simple-rag.png)

2.  2つ目のパターンは、検索拡張生成を使用するものです。これは、探している回答や情報が含まれている可能性が高い、関連するコンテキストをできるだけ多く含む質問を連結する最初のパターンよりも改善されています。
    ここで課題となるのは、使用できるコンテキスト情報の量に制限があることです。この制限は、モデルのトークンの制限によって決まります。
    ![RAG Q\&A](./images/52-rag-with-external-data.png)

これは、検索拡張生成 (RAG) を使用することで解決できます。

## 検索拡張生成 (RAG) の仕組み

RAGは、埋め込みを使用してドキュメントのコーパスにインデックスを付けてナレッジベースを構築し、LLMを使用してナレッジベース内のドキュメントのサブセットから情報を抽出することを組み合わせています。

RAGの準備ステップとして、ナレッジベースを構築するドキュメントを固定サイズ（選択した埋め込みモデルの最大入力サイズに合わせる）のチャンクに分割し、モデルに渡して埋め込みベクトルを取得します。埋め込みは、ドキュメントの元のチャンクと追加のメタデータとともに、ベクターデータベースに保存されます。ベクターデータベースは、ベクター間の類似性検索を効率的に実行できるように最適化されています。

## ターゲットオーディエンス

プライベートなデータストアや頻繁に変更されるデータストアをお持ちのお客様。RAG アプローチは 2 つの問題を解決します。次のような課題を抱えているお客様には、このラボのメリットがあります。

*   データの鮮度:データが絶えず変化していて、モデルが最新の情報のみを提供しなければならない場合。
*   知識の実態：モデルが理解できないドメイン固有の知識があり、モデルがドメインデータに従って出力する必要がある場合。

## 目標

このモジュールを修了すると、次のことをよく理解できるようになります。

1.  QA パターンとは何か、また QA パターンが検索拡張生成 (RAG) をどのように活用しているか
2.  Bedrockを使用してQ\&A RAGソリューションを実装する方法

このモジュールでは、BedrockでQAパターンを実装する方法を説明します。
さらに、ベクターデータベースに読み込まれる埋め込みも用意しています。

Titan Embeddingsを使用してユーザーの質問の埋め込みを取得し、その埋め込みを使用してベクターデータベースから最も関連性の高いドキュメントを取得し、上位3つのドキュメントを連結するプロンプトを作成し、Bedrock経由でLLMモデルを呼び出すことができることに注意してください。

## ノートブック

1.  [モデルに関する知識と簡単なコンテキストを含む Q&A](./00_qa_w_bedrock_titan.ipynb)

2.  [ラグとの質疑応答](./01_qa_w_rag_claude.ipynb)
