{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af3f88dd-0f5e-427e-84ee-8934982300d1",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Bedrock with LangChain using a Prompt that includes Context\n",
    "\n",
    "> *This notebook should work well with the **`Data Science 3.0`** kernel in SageMaker Studio*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b920ca4a-a71d-4630-a6e4-577d95192ad1",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this notebook we show you how to generate an email response to a customer who was not happy with the quality of customer service that they received from the customer support engineer. We will provide additional context to the model by providing the contents of the actual email that was received from the unhappy customer.\n",
    "\n",
    "Because of additional context in the prompt, the text produced by the [Mistral 7B](https://mistral.ai/technology/#models) Large language model in this notebook is of much better quality and relevance than the content produced earlier through zero-shot prompts.\n",
    "\n",
    "[LangChain](https://python.langchain.com/docs/get_started/introduction.html) is a framework for developing applications powered by language models. The key aspects of this framework allow us to augment the Large Language Models by chaining together various components to create advanced use cases.\n",
    "\n",
    "In this notebook we will use the Bedrock API provided by LangChain. The prompt used in this example creates a custom LangChain prompt template for adding context to the text generation request. \n",
    "\n",
    "**Note:** *This notebook can be run within or outside of AWS environment.*\n",
    "\n",
    "#### Context\n",
    "In the previous example `01_zero_shot_generation_llama2.ipynb`, we explored how to use LangChain framework to communicate with Amazon Bedrock API. In this notebook we will try to add a bit more complexity with the help of `PromptTemplates` to leverage the LangChain framework for the similar use case. `PrompTemplates` allow you to create generic shells which can be populated with information later and get model outputs based on different scenarios.\n",
    "\n",
    "As part of this notebook we will explore the use of Amazon Bedrock integration within LangChain framework and how it could be used to generate text with the help of `PromptTemplate`.\n",
    "\n",
    "#### Pattern\n",
    "We will simply provide the LangChain implementation of Amazon Bedrock API with an input consisting of a task, an instruction and an input for the model under the hood to generate an output without providing any additional example. The purpose here is to demonstrate how the powerful LLMs easily understand the task at hand and generate compelling outputs.\n",
    "\n",
    "![](./images/bedrock_langchain.jpg)\n",
    "\n",
    "#### Use case\n",
    "To demonstrate the generation capability of models in Amazon Bedrock, let's take the use case of email generation.\n",
    "\n",
    "#### Persona\n",
    "You are Bob a Customer Service Manager at AnyCompany and some of your customers are not happy with the customer service and are providing negative feedbacks on the service provided by customer support engineers. Now, you would like to respond to those customers humbly aplogizing for the poor service and regain trust. You need the help of an LLM to generate a bulk of emails for you which are human friendly and personalized to the customer's sentiment from previous email correspondence.\n",
    "\n",
    "#### Implementation\n",
    "To fulfill this use case, we will show you how to generate an email with a thank you note based on the customer's previous email. We will use the Meta Llama2 Text Large model using the Amazon Bedrock LangChain integration. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa11828a-243d-4808-9c92-e8caf4cebd37",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "⚠️ ⚠️ ⚠️ Before running this notebook, ensure you've run the [Bedrock boto3 setup notebook](../00_Intro/bedrock_boto3_setup.ipynb#Prerequisites) notebook and you have installed [langchain](https://pypi.org/project/langchain/) version 0.1.12 or higher. ⚠️ ⚠️ ⚠️\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5762bb2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: langchain==0.1.12 in /Users/snghigf/opt/anaconda3/envs/bedrock_env/lib/python3.10/site-packages (0.1.12)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /Users/snghigf/opt/anaconda3/envs/bedrock_env/lib/python3.10/site-packages (from langchain==0.1.12) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /Users/snghigf/opt/anaconda3/envs/bedrock_env/lib/python3.10/site-packages (from langchain==0.1.12) (2.0.28)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /Users/snghigf/opt/anaconda3/envs/bedrock_env/lib/python3.10/site-packages (from langchain==0.1.12) (3.9.3)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /Users/snghigf/opt/anaconda3/envs/bedrock_env/lib/python3.10/site-packages (from langchain==0.1.12) (4.0.3)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /Users/snghigf/opt/anaconda3/envs/bedrock_env/lib/python3.10/site-packages (from langchain==0.1.12) (0.5.14)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/snghigf/opt/anaconda3/envs/bedrock_env/lib/python3.10/site-packages (from langchain==0.1.12) (1.33)\n",
      "Requirement already satisfied: langchain-community<0.1,>=0.0.28 in /Users/snghigf/opt/anaconda3/envs/bedrock_env/lib/python3.10/site-packages (from langchain==0.1.12) (0.0.28)\n",
      "Requirement already satisfied: langchain-core<0.2.0,>=0.1.31 in /Users/snghigf/opt/anaconda3/envs/bedrock_env/lib/python3.10/site-packages (from langchain==0.1.12) (0.1.31)\n",
      "Requirement already satisfied: langchain-text-splitters<0.1,>=0.0.1 in /Users/snghigf/opt/anaconda3/envs/bedrock_env/lib/python3.10/site-packages (from langchain==0.1.12) (0.0.1)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /Users/snghigf/opt/anaconda3/envs/bedrock_env/lib/python3.10/site-packages (from langchain==0.1.12) (0.1.23)\n",
      "Requirement already satisfied: numpy<2,>=1 in /Users/snghigf/opt/anaconda3/envs/bedrock_env/lib/python3.10/site-packages (from langchain==0.1.12) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /Users/snghigf/opt/anaconda3/envs/bedrock_env/lib/python3.10/site-packages (from langchain==0.1.12) (2.6.3)\n",
      "Requirement already satisfied: requests<3,>=2 in /Users/snghigf/opt/anaconda3/envs/bedrock_env/lib/python3.10/site-packages (from langchain==0.1.12) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /Users/snghigf/opt/anaconda3/envs/bedrock_env/lib/python3.10/site-packages (from langchain==0.1.12) (8.2.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/snghigf/opt/anaconda3/envs/bedrock_env/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.12) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/snghigf/opt/anaconda3/envs/bedrock_env/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.12) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/snghigf/opt/anaconda3/envs/bedrock_env/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.12) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/snghigf/opt/anaconda3/envs/bedrock_env/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.12) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/snghigf/opt/anaconda3/envs/bedrock_env/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.12) (1.9.4)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Users/snghigf/opt/anaconda3/envs/bedrock_env/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.1.12) (3.21.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /Users/snghigf/opt/anaconda3/envs/bedrock_env/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.1.12) (0.9.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/snghigf/opt/anaconda3/envs/bedrock_env/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain==0.1.12) (2.4)\n",
      "Requirement already satisfied: anyio<5,>=3 in /Users/snghigf/opt/anaconda3/envs/bedrock_env/lib/python3.10/site-packages (from langchain-core<0.2.0,>=0.1.31->langchain==0.1.12) (4.3.0)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in /Users/snghigf/opt/anaconda3/envs/bedrock_env/lib/python3.10/site-packages (from langchain-core<0.2.0,>=0.1.31->langchain==0.1.12) (23.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /Users/snghigf/opt/anaconda3/envs/bedrock_env/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.17->langchain==0.1.12) (3.9.15)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/snghigf/opt/anaconda3/envs/bedrock_env/lib/python3.10/site-packages (from pydantic<3,>=1->langchain==0.1.12) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in /Users/snghigf/opt/anaconda3/envs/bedrock_env/lib/python3.10/site-packages (from pydantic<3,>=1->langchain==0.1.12) (2.16.3)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /Users/snghigf/opt/anaconda3/envs/bedrock_env/lib/python3.10/site-packages (from pydantic<3,>=1->langchain==0.1.12) (4.10.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/snghigf/opt/anaconda3/envs/bedrock_env/lib/python3.10/site-packages (from requests<3,>=2->langchain==0.1.12) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/snghigf/opt/anaconda3/envs/bedrock_env/lib/python3.10/site-packages (from requests<3,>=2->langchain==0.1.12) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/snghigf/opt/anaconda3/envs/bedrock_env/lib/python3.10/site-packages (from requests<3,>=2->langchain==0.1.12) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/snghigf/opt/anaconda3/envs/bedrock_env/lib/python3.10/site-packages (from requests<3,>=2->langchain==0.1.12) (2024.2.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /Users/snghigf/opt/anaconda3/envs/bedrock_env/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain==0.1.12) (3.0.3)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Users/snghigf/opt/anaconda3/envs/bedrock_env/lib/python3.10/site-packages (from anyio<5,>=3->langchain-core<0.2.0,>=0.1.31->langchain==0.1.12) (1.3.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /Users/snghigf/opt/anaconda3/envs/bedrock_env/lib/python3.10/site-packages (from anyio<5,>=3->langchain-core<0.2.0,>=0.1.31->langchain==0.1.12) (1.2.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/snghigf/opt/anaconda3/envs/bedrock_env/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain==0.1.12) (1.0.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain==0.1.12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "558a9372-0789-414a-a1d7-2976056f2015",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import boto3\n",
    "import botocore\n",
    "\n",
    "module_path = \"..\"\n",
    "sys.path.append(os.path.abspath(module_path))\n",
    "from utils import bedrock, print_ww\n",
    "\n",
    "bedrock_client = boto3.client('bedrock-runtime')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7daa1a8-d21a-410c-adbf-b253c2dabf80",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Invoke the Bedrock LLM Model\n",
    "\n",
    "We'll begin with creating an instance of Bedrock class from llms. This expects a `model_id` which is the ARN of the model available in Amazon Bedrock. \n",
    "\n",
    "Optionally you can pass on a previously created boto3 client as well as some `model_kwargs` which can hold parameters such as `temperature`, `topP`, `maxTokenCount` or `stopSequences` (more on parameters can be explored in Amazon Bedrock console).\n",
    "\n",
    "Check [documentation](https://docs.aws.amazon.com/bedrock/latest/userguide/model-ids-arns.html) for Available text generation model Ids under Amazon Bedrock.\n",
    "\n",
    "Note that different models support different `model_kwargs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ffa1250-56cd-4b6d-b3d8-c62baac143ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.llms.bedrock import Bedrock\n",
    "\n",
    "inference_params = {'max_tokens':500, \n",
    "                      \"temperature\":0.5,\n",
    "                      \"top_k\":50,\n",
    "                      \"top_p\":0.9,\n",
    "                     }\n",
    "\n",
    "llm = Bedrock(\n",
    "    model_id=\"mistral.mistral-7b-instruct-v0:2\",\n",
    "    model_kwargs=inference_params\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2678ed-f0d6-444f-9a57-5170dd1952f7",
   "metadata": {},
   "source": [
    "## Create a LangChain custom prompt template\n",
    "\n",
    "By creating a template for the prompt we can pass it different input variables to it on every run. This is useful when you have to generate content with different input variables that you may be fetching from a database.\n",
    "\n",
    "Previously we hardcoded the prompt, it might be the case that you have multiple customers sending similar negative feedback and you now want to use each of those customer's emails and respond to them with an apology but you also want to keep the response a bit personalized. In the following cell we are exploring how you can create a `PromptTemplate` to achieve this pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dbec103a-97ae-4e9e-9d80-dc20f354a228",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Create a prompt template that has multiple input variables\n",
    "multi_var_prompt = PromptTemplate(\n",
    "    input_variables=[\"customerServiceManager\", \"customerName\", \"feedbackFromCustomer\"], \n",
    "    template=\"\"\"\n",
    "\n",
    "Create an apology email from the Service Manager {customerServiceManager} to {customerName} in response to the following feedback that was received from the customer: \n",
    "<customer_feedback>\n",
    "{feedbackFromCustomer}\n",
    "</customer_feedback>\n",
    "\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# Pass in values to the input variables\n",
    "prompt = multi_var_prompt.format(customerServiceManager=\"Bob\", \n",
    "                                 customerName=\"John Doe\", \n",
    "                                 feedbackFromCustomer=\"\"\"Hello Bob,\n",
    "     I am very disappointed with the recent experience I had when I called your customer support.\n",
    "     I was expecting an immediate call back but it took three days for us to get a call back.\n",
    "     The first suggestion to fix the problem was incorrect. Ultimately the problem was fixed after three days.\n",
    "     We are very unhappy with the response provided and may consider taking our business elsewhere.\n",
    "     \"\"\"\n",
    "     )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e45bdfd5-ce76-42e9-81cd-b0892337d163",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our prompt has 154 tokens\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models.bedrock import ChatPromptAdapter\n",
    "from langchain.schema import HumanMessage\n",
    "\n",
    "msg = HumanMessage(content=prompt)\n",
    "prompt = ChatPromptAdapter().convert_messages_to_prompt('mistral', [msg])\n",
    "\n",
    "num_tokens = llm.get_num_tokens(prompt)\n",
    "print(f\"Our prompt has {num_tokens} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1bf31e9-56c0-408f-a652-9e23de446aef",
   "metadata": {},
   "source": [
    "## Invoke again\n",
    "\n",
    "invoke using the prompt template and expect to see a curated response back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c1064c57-27a4-48c5-911b-e4f1dfeff122",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dear John Doe,\n",
      "\n",
      "I hope this email finds you well. I was deeply saddened to read your feedback regarding your recent\n",
      "interaction with our customer support team. I want to personally apologize for the long wait time\n",
      "for a call back and the incorrect initial suggestion to fix your issue.\n",
      "\n",
      "At our company, we pride ourselves on providing excellent customer service, and it is clear that we\n",
      "fell short in your case. I understand your frustration and disappointment, and I want to assure you\n",
      "that we take your concerns seriously.\n",
      "\n",
      "I have looked into the matter and have identified some areas where we could have improved our\n",
      "response time and the accuracy of our initial diagnosis. I want to assure you that we are taking\n",
      "immediate steps to address these issues and prevent similar occurrences in the future.\n",
      "\n",
      "Furthermore, I would like to make it right for you. I would like to offer you a discount on your\n",
      "next purchase or service with us as a gesture of goodwill. Please let me know if this is something\n",
      "that would be acceptable to you.\n",
      "\n",
      "Once again, please accept my sincere apologies for the inconvenience caused. I value your business\n",
      "and would hate to see you go. I would be grateful if you could give me the opportunity to make\n",
      "things right for you.\n",
      "\n",
      "If you have any further concerns or feedback, please do not hesitate to contact me directly at\n",
      "[bob.smith@company.com] or [phone number].\n",
      "\n",
      "Thank you for bringing this matter to our attention. I hope that we can continue to work together to\n",
      "resolve any issues you may have and to ensure that your future experiences with our company are\n",
      "positive ones.\n",
      "\n",
      "Best regards,\n",
      "\n",
      "Bob, Service Manager\n",
      "[Company Name]\n"
     ]
    }
   ],
   "source": [
    "response = llm(prompt)\n",
    "\n",
    "email = response[response.index('\\n')+1:]\n",
    "\n",
    "print_ww(email)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9abc40",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "To conclude we learnt that invoking the LLM without any context might not yield the desired results. By adding context and further using the the prompt template to constrain the output from the LLM we are able to successfully get our desired output"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
